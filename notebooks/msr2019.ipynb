{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dataset for MSR 2019 (data showcase paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This points to the dataset CSV file (leave out the '_release.csv' suffix!)\n",
    "VULAS_DB_NAME='../dataset/vulas_db_msr2019_8'\n",
    "\n",
    "GIT_REPO_FOLDER = '/tmp/git-cache-3'\n",
    "SKIP_CLONE_EXISTING=True\n",
    "\n",
    "# This controls how many negatives you want per each positive\n",
    "RATIO_POS_NEG = 1\n",
    "DATASET_NAME='dataset_msr2019_8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys; sys.path.append(os.path.join(sys.path[0],'acacia'))\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "import requests_cache\n",
    "requests_cache.install_cache('requests_cache', expire_after=7 * 24*60*60)\n",
    "\n",
    "# acacia stuff\n",
    "from utils import *\n",
    "from acacia.git import clone_repo, clone_repo_multiple, extract_commit_msg, extract_commit_diff, get_random_commits\n",
    "from acacia.git_temporal import extract_timing_data\n",
    "\n",
    "POS_CLS='pos'\n",
    "NEG_CLS='neg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LatexExporter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset from CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains 4 commits\n"
     ]
    }
   ],
   "source": [
    "with open(VULAS_DB_NAME + '_release.csv','r') as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "vulas_db_content=[]\n",
    "for d in data:\n",
    "    line = d.strip().split(',')\n",
    "    vulas_db_content.append(tuple(line))\n",
    "\n",
    "# len(vulas_db_content)\n",
    "print('Dataset contains {} commits'.format(len(vulas_db_content)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloning\n",
    "\n",
    "This clones locally all (external git) repositories that appear in the Vulas DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "repos_set = set([ r for _,r,_,_ in vulas_db_content ])\n",
    "clone_repo_multiple(repos_set,\n",
    "       output_folder=GIT_REPO_FOLDER,\n",
    "       proxy='',\n",
    "       skip_existing=SKIP_CLONE_EXISTING,\n",
    "       concurrent=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing commit data in a dataframe (positive instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:00<00:01,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('v2.4.5.RELEASE', '1496240700', '1496240700', 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 2/4 [00:01<00:01,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('4.0.5', '1521443138', '1520616563', 826575)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 3/4 [00:02<00:00,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('camel-2.16.5', '1481903857', '1481744113', 159744)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 4/4 [00:03<00:00,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('jenkins-2.84', '1507571749', '1505309456', 2262293)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def extract_commit(db_content, verbose=False):\n",
    "    ''' return a dataframe with commit data '''\n",
    "    data = []\n",
    "    for vuln_id, repo, commit, cls in tqdm(db_content):\n",
    "        try:\n",
    "            commit_msg = extract_commit_msg(commit, repo, GIT_REPO_FOLDER) \n",
    "            commit_diff = extract_commit_diff(commit, repo, GIT_REPO_FOLDER)\n",
    "            fix_tag, fix_tag_timestamp, commit_timestamp, commit_tag_delay = extract_timing_data(commit, repo, verbose, GIT_REPO_FOLDER)\n",
    "            data.append({\n",
    "                'msg' : commit_msg.decode(\"latin-1\"),\n",
    "                'diff': commit_diff.decode(\"latin-1\"),\n",
    "                'cls': cls,\n",
    "                'commit_id': commit,\n",
    "                'repo_url': repo,\n",
    "                'vuln_id': vuln_id,\n",
    "                'in_nvd': False,\n",
    "                'fix_tag': fix_tag,\n",
    "                'fix_tag_timestamp': int(fix_tag_timestamp),\n",
    "                'commit_timestamp' : int(commit_timestamp),\n",
    "                'commit_tag_delay': int(commit_tag_delay)\n",
    "            })\n",
    "        except Exception as e:\n",
    "            pass\n",
    "            print('Skipping %s' % repo)\n",
    "            print(e)\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "df_pos = extract_commit(vulas_db_content, verbose=False)\n",
    "df_pos.to_pickle(DATASET_NAME + '_pos.pd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 4 (pos) commits.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cls</th>\n",
       "      <th>commit_id</th>\n",
       "      <th>commit_tag_delay</th>\n",
       "      <th>commit_timestamp</th>\n",
       "      <th>diff</th>\n",
       "      <th>fix_tag</th>\n",
       "      <th>fix_tag_timestamp</th>\n",
       "      <th>in_nvd</th>\n",
       "      <th>msg</th>\n",
       "      <th>repo_url</th>\n",
       "      <th>vuln_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pos</td>\n",
       "      <td>57f2ccb66946943fbf3b3f2165eac1c8eb6b1523</td>\n",
       "      <td>0</td>\n",
       "      <td>1496240700</td>\n",
       "      <td>diff --git a/spring-webflow/src/main/java/org/...</td>\n",
       "      <td>v2.4.5.RELEASE</td>\n",
       "      <td>1496240700</td>\n",
       "      <td>False</td>\n",
       "      <td>Use fixed parser for empty value binding expre...</td>\n",
       "      <td>https://github.com/spring-projects/spring-webflow</td>\n",
       "      <td>CVE-2017-4971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pos</td>\n",
       "      <td>8471904a02438c03965d21367890276bc25fa5a6</td>\n",
       "      <td>826575</td>\n",
       "      <td>1520616563</td>\n",
       "      <td>diff --git a/docs/release-notes.html b/docs/re...</td>\n",
       "      <td>4.0.5</td>\n",
       "      <td>1521443138</td>\n",
       "      <td>False</td>\n",
       "      <td>Fix a SimpleBindRequest bug\\n</td>\n",
       "      <td>https://github.com/pingidentity/ldapsdk</td>\n",
       "      <td>CVE-2018-1000134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>57d01e2fc8923263df896e9810329ee5b7f9b69</td>\n",
       "      <td>159744</td>\n",
       "      <td>1481744113</td>\n",
       "      <td>diff --git a/components/camel-jackson/src/main...</td>\n",
       "      <td>camel-2.16.5</td>\n",
       "      <td>1481903857</td>\n",
       "      <td>False</td>\n",
       "      <td>CAMEL-10567: Camel-Jackson: Add an option to a...</td>\n",
       "      <td>https://github.com/apache/camel</td>\n",
       "      <td>CVE-2016-8749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pos</td>\n",
       "      <td>d7ea3f40efedd50541a57b943d5f7bbed046d091</td>\n",
       "      <td>2262293</td>\n",
       "      <td>1505309456</td>\n",
       "      <td>diff --git a/core/src/main/java/hudson/slaves/...</td>\n",
       "      <td>jenkins-2.84</td>\n",
       "      <td>1507571749</td>\n",
       "      <td>False</td>\n",
       "      <td>[SECURITY-478] Require RUN_SCRIPTS before conf...</td>\n",
       "      <td>https://github.com/jenkinsci/jenkins/</td>\n",
       "      <td>CVE-2017-1000393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cls                                 commit_id  commit_tag_delay  \\\n",
       "0  pos  57f2ccb66946943fbf3b3f2165eac1c8eb6b1523                 0   \n",
       "1  pos  8471904a02438c03965d21367890276bc25fa5a6            826575   \n",
       "2  pos   57d01e2fc8923263df896e9810329ee5b7f9b69            159744   \n",
       "3  pos  d7ea3f40efedd50541a57b943d5f7bbed046d091           2262293   \n",
       "\n",
       "   commit_timestamp                                               diff  \\\n",
       "0        1496240700  diff --git a/spring-webflow/src/main/java/org/...   \n",
       "1        1520616563  diff --git a/docs/release-notes.html b/docs/re...   \n",
       "2        1481744113  diff --git a/components/camel-jackson/src/main...   \n",
       "3        1505309456  diff --git a/core/src/main/java/hudson/slaves/...   \n",
       "\n",
       "          fix_tag  fix_tag_timestamp  in_nvd  \\\n",
       "0  v2.4.5.RELEASE         1496240700   False   \n",
       "1           4.0.5         1521443138   False   \n",
       "2    camel-2.16.5         1481903857   False   \n",
       "3    jenkins-2.84         1507571749   False   \n",
       "\n",
       "                                                 msg  \\\n",
       "0  Use fixed parser for empty value binding expre...   \n",
       "1                      Fix a SimpleBindRequest bug\\n   \n",
       "2  CAMEL-10567: Camel-Jackson: Add an option to a...   \n",
       "3  [SECURITY-478] Require RUN_SCRIPTS before conf...   \n",
       "\n",
       "                                            repo_url           vuln_id  \n",
       "0  https://github.com/spring-projects/spring-webflow     CVE-2017-4971  \n",
       "1            https://github.com/pingidentity/ldapsdk  CVE-2018-1000134  \n",
       "2                    https://github.com/apache/camel     CVE-2016-8749  \n",
       "3              https://github.com/jenkinsci/jenkins/  CVE-2017-1000393  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pos = pd.read_pickle(DATASET_NAME + '_pos.pd')\n",
    "if len(vulas_db_content) - len(df_pos.index) != 0:\n",
    "    print('There where ' + str(len(vulas_db_content)- len(df_pos.index))  + ' commits that could not be processed')\n",
    "print('Successfully processed ' + str(len(df_pos.index)) + ' (pos) commits.')\n",
    "df_pos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FIXBELOWHANDREDDAYS] Number of fix commits released in less than 100 days: 3\n",
      "[FIXBELOWONEDAYS] Number of fix commits released in less than 1 days: 0\n",
      "[FIXNOTRELEASED] Number of fix commits not released: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, '# Commits')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_pos['days']=df_pos[df_pos['commit_tag_delay'] > 0]['commit_tag_delay']/(3600*24)\n",
    "\n",
    "df1=df_pos[df_pos['commit_tag_delay'] > 0]['commit_tag_delay']/(3600*24)\n",
    "\n",
    " \n",
    "\n",
    "df2=df1.to_frame()\n",
    "\n",
    " \n",
    "\n",
    "le.save('FIXBELOWHANDREDDAYS', len(df2[(df2['commit_tag_delay']).round()<100]), 'Number of fix commits released in less than 100 days')\n",
    "\n",
    "le.save('FIXBELOWONEDAYS', (len(df2[(df2['commit_tag_delay']).round() <1 ]) ), 'Number of fix commits released in less than 1 days')\n",
    "\n",
    "le.save('FIXNOTRELEASED', len(df_pos[df_pos['days']==np.nan]), 'Number of fix commits not released')\n",
    "\n",
    " \n",
    "\n",
    "d = {'Days': [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,50,100,500,1000,2000 ],\n",
    "     '# Commits': [len(df2[df2['commit_tag_delay'].round()<1]),\n",
    "        len(df2[df2['commit_tag_delay'].round()>=1])&len(df2[df2['commit_tag_delay'].round()<2]),\n",
    "        len(df2[df2['commit_tag_delay'].round()>=2])&len(df2[df2['commit_tag_delay'].round()<3]),\n",
    "        len(df2[df2['commit_tag_delay'].round()>=3])&len(df2[df2['commit_tag_delay'].round()<4]),\n",
    "        len(df2[df2['commit_tag_delay'].round()>=4])&len(df2[df2['commit_tag_delay'].round()<5]),\n",
    "        len(df2[df2['commit_tag_delay'].round()>=5])&len(df2[df2['commit_tag_delay'].round()<6]),\n",
    "        len(df2[df2['commit_tag_delay'].round()>=6])&len(df2[df2['commit_tag_delay'].round()<7]),\n",
    "        len(df2[df2['commit_tag_delay'].round()>=7])&len(df2[df2['commit_tag_delay'].round()<8]),\n",
    "        len(df2[df2['commit_tag_delay'].round()>=8])&len(df2[df2['commit_tag_delay'].round()<9]),\n",
    "        len(df2[df2['commit_tag_delay'].round()>=9])&len(df2[df2['commit_tag_delay'].round()<10]),\n",
    "        len(df2[df2['commit_tag_delay'].round()>=10])&len(df2[df2['commit_tag_delay'].round()<11]),\n",
    "        len(df2[df2['commit_tag_delay'].round()>=11])&len(df2[df2['commit_tag_delay'].round()<12]),\n",
    "        len(df2[df2['commit_tag_delay'].round()>=12])&len(df2[df2['commit_tag_delay'].round()<13]),\n",
    "        len(df2[df2['commit_tag_delay'].round()>=13])&len(df2[df2['commit_tag_delay'].round()<14]),\n",
    "        len(df2[df2['commit_tag_delay'].round()>=14])&len(df2[df2['commit_tag_delay'].round()<15]),\n",
    "        len(df2[df2['commit_tag_delay'].round()>=15])&len(df2[df2['commit_tag_delay'].round()<50]),\n",
    "        len(df2[df2['commit_tag_delay'].round()>=50])&len(df2[df2['commit_tag_delay'].round()<100]),\n",
    "        len(df2[df2['commit_tag_delay'].round()>=100])&len(df2[df2['commit_tag_delay'].round()<500]),\n",
    "        len(df2[df2['commit_tag_delay'].round()>=500])&len(df2[df2['commit_tag_delay'].round()<1000]),\n",
    "        len(df2[df2['commit_tag_delay'].round()>=1000])&len(df2[df2['commit_tag_delay'].round()<2000])\n",
    "]}\n",
    "\n",
    "df = pd.DataFrame(data=d)\n",
    "\n",
    "   \n",
    "\n",
    "with open('msr2019/tex/delays'+'.tex','w') as tf:\n",
    "\n",
    "        tf.write(df.to_latex(columns=['Days','# Commits'],index=False, column_format='lc'))\n",
    "\n",
    "   \n",
    "\n",
    " \n",
    "\n",
    "plt.savefig('msr2019/img/commitTagDelay'+'.pdf', dpi=200, bbox_inches='tight')\n",
    "matplotlib.pyplot.hist(df1.round(),bins=20,log=True,histtype='bar')\n",
    "matplotlib.pyplot.xlabel(\"Days\")\n",
    "matplotlib.pyplot.ylabel(\"# Commits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create deduplicated version of (pos) dataset\n",
    "\n",
    "Deduplication is based on the equality of commit messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 duplicate commits out of 4\n",
      "[DUPLICATECOMMITSCOUNT] Number of duplicate commits: 1\n"
     ]
    }
   ],
   "source": [
    "# count duplicates\n",
    "\n",
    "dup_idx = df_pos.duplicated(subset='msg')\n",
    "if(len(dup_idx.value_counts())>1):\n",
    "    dup_commit_count = dup_idx.value_counts()[1]\n",
    "print('There are {} duplicate commits out of {}'.format(dup_commit_count, len(df_pos.index)))\n",
    "\n",
    "le.save('DUPLICATECOMMITSCOUNT', dup_commit_count, 'Number of duplicate commits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cls</th>\n",
       "      <th>commit_id</th>\n",
       "      <th>commit_tag_delay</th>\n",
       "      <th>commit_timestamp</th>\n",
       "      <th>diff</th>\n",
       "      <th>fix_tag</th>\n",
       "      <th>fix_tag_timestamp</th>\n",
       "      <th>in_nvd</th>\n",
       "      <th>msg</th>\n",
       "      <th>repo_url</th>\n",
       "      <th>vuln_id</th>\n",
       "      <th>days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [cls, commit_id, commit_tag_delay, commit_timestamp, diff, fix_tag, fix_tag_timestamp, in_nvd, msg, repo_url, vuln_id, days]\n",
       "Index: []"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pos_dedup = df_pos.drop_duplicates(subset='msg')\n",
    "df_pos[df_pos.duplicated(subset='msg')].vuln_id.nunique()\n",
    "# df_pos_dedup.vuln_id.nunique()\n",
    "df_pos[dup_idx].sort_values('msg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DATASETSIZEPOS] Number of 'pos' commits in the dataset (this includes duplicates!): 4\n",
      "[DATASETSIZEPOSDEDUP] Number of pos commits, **de-duplicated**: 4\n"
     ]
    }
   ],
   "source": [
    "le.save('DATASETSIZEPOS', len(df_pos.index), 'Number of \\'pos\\' commits in the dataset (this includes duplicates!)' )\n",
    "le.save('DATASETSIZEPOSDEDUP', len(df_pos_dedup), 'Number of pos commits, **de-duplicated**')\n",
    "# le.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expand dataset with 'negative' instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:04<00:00,  1.05s/it]\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# from acacia.git import *\n",
    "# # from acacia.dataset import *\n",
    "# from tqdm import tqdm\n",
    "\n",
    "import random\n",
    "RND_SEED=17534532\n",
    "\n",
    "def expand_dataset(repos_set, df_pos):\n",
    "    neg_instances = []\n",
    "    SEC_REGEXP = '(denial.of.service|XXE|remote.code.execution|open.redirect|osvdb|secur.+|vuln.+|exploit.+|malicious.*|attack.*|dos|directory.traversal|injection|cve-\\d+-\\d+|xss|nvd|cross.site|csrf|rce|clickjack|session.fixation|advisory|insecur.+|cross.origin|unauthori[z|s].+)'\n",
    "\n",
    "    for repo in tqdm(repos_set):\n",
    "        count = len(df_pos.loc[df_pos['repo_url'] == repo])\n",
    "        commits = get_random_commits(int(count * RATIO_POS_NEG), repo, GIT_REPO_FOLDER)\n",
    "        for c in commits:\n",
    "            try:\n",
    "                new_instance = {}\n",
    "                commit_msg = extract_commit_msg(c, repo, GIT_REPO_FOLDER)\n",
    "                commit_diff = extract_commit_diff(c, repo, GIT_REPO_FOLDER) \n",
    "                if re.match(SEC_REGEXP, commit_msg.decode(\"utf-8\")):\n",
    "                    print('Found match of SEC_REGEXP in NEG instance')\n",
    "                    print(commit_msg)\n",
    "\n",
    "                new_instance = {\n",
    "                    'msg' : commit_msg.decode(\"latin-1\") ,\n",
    "                    'diff': commit_diff.decode(\"latin-1\") ,\n",
    "                    'cls': 'neg',\n",
    "                    'commit_id': c,\n",
    "                    'repo_url': repo ,\n",
    "                    'vuln_id' : '',\n",
    "                    'in_nvd': False,\n",
    "                    'fix_tag': '',\n",
    "                    'fix_tag_timestamp': 0,\n",
    "                    'commit_timestamp' : 0,\n",
    "                    'commit_tag_delay': 0\n",
    "                }\n",
    "                neg_instances.append(new_instance)\n",
    "            except Exception as e:\n",
    "                print('There was an exception with this commit, skipping: %s:%s' % (repo,c) )\n",
    "                print(e)\n",
    "    return pd.DataFrame(neg_instances)\n",
    "\n",
    "random.seed(RND_SEED)\n",
    "df_neg = expand_dataset(repos_set,df_pos_dedup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 4 commits\n"
     ]
    }
   ],
   "source": [
    "df_neg = pd.DataFrame()\n",
    "df_all = pd.concat([df_pos_dedup, df_neg], ignore_index = True)\n",
    "overall_size = len(df_all.index)\n",
    "print('The dataset contains '+ str(overall_size) + ' commits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_pickle(DATASET_NAME + '_all.pd')\n",
    "df_pos.to_pickle(DATASET_NAME + '_pos.pd')\n",
    "df_pos_dedup.to_pickle(DATASET_NAME + '_pos_dedup.pd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[['repo_url','commit_id','cls']].to_csv(DATASET_NAME + '.csv')\n",
    "df_pos_dedup[['repo_url','commit_id','cls']].to_csv(DATASET_NAME + '_pos_dedup.csv')\n",
    "df_pos[['repo_url','commit_id','cls']].to_csv(DATASET_NAME + '_pos.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos = pd.read_pickle(DATASET_NAME + '_pos.pd')\n",
    "df_pos_dedup = pd.read_pickle(DATASET_NAME + '_pos_dedup.pd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VULNLOSSDEDUP] Number of vulnerabilities removed by the deduplication as they have the same fix commits than other vulnerabilities: 0\n"
     ]
    }
   ],
   "source": [
    " def diff(first, second):\n",
    "        second = set(second)\n",
    "        return [item for item in first if item not in second]\n",
    "\n",
    "le.save('VULNLOSSDEDUP',\n",
    "            len(diff(set(df_pos['vuln_id'].values),set(df_pos_dedup['vuln_id'].values))),\n",
    "            'Number of vulnerabilities removed by the deduplication as they have the same fix commits than other vulnerabilities ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_counts(df, suffix=''):\n",
    "    print(\"Dataset size:           {}\".format(len(df.index)))\n",
    "    print(\"Instances of class POS: {}\".format(len(df[df['cls'] == POS_CLS ])))\n",
    "    print(\"Instances of class NEG: {}\".format(len(df[df['cls'] == NEG_CLS ])))\n",
    "\n",
    "    le.save('COMMITCOUNT' + suffix,\n",
    "            len(df[df['cls'] == POS_CLS ]),\n",
    "            'Number of commits ' + suffix)\n",
    "    \n",
    "    le.save('VULNCOUNT'+suffix,\n",
    "            df[df['cls'] == POS_CLS ].vuln_id.nunique(),\n",
    "            'Unique vulnerabilities (after mapping vulns) ' + suffix)\n",
    "\n",
    "    le.save('REPOCOUNT'+suffix,\n",
    "            df[df['cls'] == POS_CLS ].repo_url.nunique(),\n",
    "            'Number of unique repositories (after removing internal, dead, svn). ' + suffix)\n",
    "    \n",
    "    df.head()\n",
    "\n",
    "print_counts(df_pos)\n",
    "print('-----------------------------------------------')\n",
    "print_counts(df_pos_dedup,'DEDUP')\n",
    "\n",
    "print('-----------------------------------------------')\n",
    "le.print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Commits per CVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printVulnXCommit(df,suffix=''):\n",
    "    df2 = df.loc[df['cls']==POS_CLS,['commit_id','vuln_id']]\n",
    "    df_vulnOnly = df2[['vuln_id']]\n",
    "    df_vulnOnly['commits'] = df_vulnOnly.groupby('vuln_id')['vuln_id'].transform('count')\n",
    "    df_vulnOnly=df_vulnOnly.drop_duplicates(subset=None, keep='first', inplace=False).sort_values(by='commits',ascending=False)\n",
    "    #print(df_vulnOnly.sort_values(by='commits',ascending=False).head(20))\n",
    "    print(\"Number of vuln with 1 commit: {}\".format(len(df_vulnOnly[df_vulnOnly['commits']==1])))\n",
    "\n",
    "\n",
    "    with open('msr2019/tex/commitXvuln'+suffix+'.tex','w') as tf:\n",
    "        tf.write(df_vulnOnly.to_latex(columns=['vuln_id','commits'],index=False, column_format='lc'))\n",
    "        \n",
    "    # Create figure\n",
    "    f, ax1 = plt.subplots(figsize=(5,3), dpi=200)\n",
    "\n",
    "    df_vulnOnly.groupby('commits').count().plot(ax=ax1, kind='bar',logy=True )\n",
    "\n",
    "    labels = df_vulnOnly.groupby('commits').count()\n",
    "    print(labels['vuln_id'].tolist())\n",
    "    #for i, v in enumerate(labels['vuln_id'].tolist()):\n",
    "    #    ax1.text(v , i + .25, str(v), color='blue', fontweight='bold')\n",
    "    \n",
    "    #ax1.legend(loc=(0.65,0.05))\n",
    "    ax1.legend().set_visible(False)\n",
    "\n",
    "    f.subplots_adjust(hspace=0.1)\n",
    "    # plt.xticks([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "\n",
    "    ax1.grid(False)\n",
    "    # ax1.set_title('msg', x=0.92, y=0.05)\n",
    "    #ax1.set_title('# Vulnerabilities', x=0.52, y=0.85)\n",
    "    plt.ylabel('# Vulnerabilities')\n",
    "    plt.xlabel('# Commits')\n",
    "\n",
    "    plt.setp([a.get_xticklabels() for a in f.axes[:-1]], visible=False)\n",
    "    plt.savefig('msr2019/img/commitXvuln'+suffix+'.pdf', dpi=200, bbox_inches='tight')\n",
    "\n",
    "printVulnXCommit(df_pos)\n",
    "print('-----------------------------------------------')\n",
    "printVulnXCommit(df_pos_dedup,'DEDUP')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of vulnerabilities per repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printvulnXrepo(df,suffix=''):\n",
    "    df1=df.loc[df['cls']==POS_CLS,['repo_url','vuln_id']]\n",
    "\n",
    "    #df1.groupby('repo_url').vuln_id.nunique()\n",
    "    df1['distinct_vuln']=df1.groupby('repo_url')['vuln_id'].transform('nunique')\n",
    "    df1.sort_values(by=['repo_url','vuln_id'])\n",
    "\n",
    "\n",
    "\n",
    "    df1=df1[['repo_url','distinct_vuln']].drop_duplicates(subset=None, keep='first', inplace=False).sort_values(by='distinct_vuln',ascending=False)\n",
    "    #df1.plot()\n",
    "    #df1.groupby('distinct_vuln').count().plot(kind='bar')\n",
    "\n",
    "    with open('msr2019/tex/vulnXrepo'+suffix+'.tex','w') as tf:\n",
    "        tf.write(df1.to_latex(columns=['repo_url','distinct_vuln'],index=False, column_format='lc'))\n",
    "        # Create figure\n",
    "    f, ax1 = plt.subplots(figsize=(5,3), dpi=200)\n",
    "    #print(df1.groupby('distinct_vuln').count())\n",
    "    df1.groupby('distinct_vuln').count().plot(ax=ax1, kind='bar',logy=True )\n",
    "\n",
    "    #ax1.legend(loc=(0.65,0.05))\n",
    "    ax1.legend().set_visible(False)\n",
    "\n",
    "    f.subplots_adjust(hspace=0.1)\n",
    "    #plt.xticks([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "\n",
    "    ax1.grid(False)\n",
    "    # ax1.set_title('msg', x=0.92, y=0.05)\n",
    "    #ax1.set_title('# Repo', x=0.52, y=0.85)\n",
    "    plt.ylabel('#Repositories')\n",
    "    plt.xlabel('#Vulnerabilities')\n",
    "\n",
    "    plt.setp([a.get_xticklabels() for a in f.axes[:-1]], visible=False)\n",
    "\n",
    "    plt.savefig('msr2019/img/repoXNumVuln'+suffix+'.pdf', dpi=200, bbox_inches='tight')\n",
    "\n",
    "printvulnXrepo(df_pos)\n",
    "printvulnXrepo(df_pos_dedup,'DEDUP')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vulnerabilities per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getYear(vuln_id):\n",
    "    LOOKUP_TABLE_YEAR = {\n",
    "        \"413684\" : \"2013\",\n",
    "        \"SONARQUBE-001\" : \"2018\",\n",
    "        \"NIFI-4436\" : \"2017\",\n",
    "        \"HADOOP-14246\" : \"2017\",\n",
    "        \"HDFS-10276\" : \"2016\",\n",
    "        \"ND4J-001\" : \"2018\",\n",
    "        \"APACHE-COMMONS-001\" : \"2018\",\n",
    "        \"HADOOP-13105\" : \"2016\",\n",
    "        \"HADOOP-12001\" : \"2015\",\n",
    "        \"HADOOP-15212\" : \"2018\",\n",
    "        \"APACHE-AXIS2-5683\" : \"2015\",\n",
    "        \"S2-028\" : \"2017\",\n",
    "        \"PLEXUS-ARCHIVER-87\" : \"2018\",\n",
    "        \"HADOOP-12751\" : \"2016\",\n",
    "        \"JAVAMELODY-252\" : \"2015\",\n",
    "        \"PT-2013-65\" : \"2013\",\n",
    "        \"SPR-7779\" : \"2010\",\n",
    "        \"HTTPCLIENT-1803\" : \"2017\",\n",
    "        \"PRIMEFACES-1194\" : \"2016\",\n",
    "        \"JAVAMELODY-631\" : \"2017\",\n",
    "        \"ZEPPELIN-2769\" : \"2017\",\n",
    "        \"APACHE-AXIS2-5846\" : \"2017\",\n",
    "        \"AMQ-5751\" : \"2015\",\n",
    "        \"AMQP-590\": \"2016\",\n",
    "        \"2012-05-05\" : \"2012\",\n",
    "        \"HUDSON-483532\" : \"2015\",\n",
    "        \"GEODE-4270\" : \"2018\",\n",
    "        \"S2-043\" : \"2016\",\n",
    "        \"JETTY-1042\" : \"2009\",\n",
    "        \"COLLECTIONS-580\": '2015',\n",
    "        \"PDFBOX-3341\":\"2016\"\n",
    "    }\n",
    "    z = re.search(r'(?<=CVE-)\\d{4}',vuln_id)\n",
    "    if(z is not None):\n",
    "        year = z.group(0)\n",
    "    else:\n",
    "        year = LOOKUP_TABLE_YEAR[vuln_id]\n",
    "    return year\n",
    "\n",
    "def checkNVD(vuln_id):\n",
    "    url = 'https://nvd.nist.gov/vuln/detail/'+vuln_id\n",
    "#     print('Checking the NVD for ' + url)\n",
    "    r = requests.get('https://nvd.nist.gov/vuln/detail/'+vuln_id)\n",
    "    if r.status_code!=200 :\n",
    "        return False\n",
    "    if 'CVE ID Not Found' in r.text:\n",
    "        return False\n",
    "    if 'Vuln ID, expected format: CVE-'  in r.text:\n",
    "        return False\n",
    "    return True\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printVulnXYear(df,suffix=''):\n",
    "    df_vuln=df.loc[df['cls']==POS_CLS,['vuln_id']].drop_duplicates(subset=None, keep='first', inplace=False)\n",
    "\n",
    "    print(\"Number of distinct vuln : {}\".format(len(df_vuln)))\n",
    "\n",
    "    df_vuln['year'] = df_vuln['vuln_id'].apply(lambda x: getYear(x))\n",
    "\n",
    "    # Create figure\n",
    "    f, ax1 = plt.subplots(figsize=(5,4), dpi=200)\n",
    "\n",
    "    df_vuln.groupby('year').count().plot(kind='bar',ax=ax1)\n",
    "\n",
    "    #ax1.legend(loc=(0.65,0.05))\n",
    "    ax1.legend().set_visible(False)\n",
    "\n",
    "    f.subplots_adjust(hspace=0.1)\n",
    "    #plt.xticks([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "\n",
    "    ax1.grid(False)\n",
    "    # ax1.set_title('msg', x=0.92, y=0.05)\n",
    "    #ax1.set_title('# Repo', x=0.52, y=0.85)\n",
    "    plt.ylabel('#Vulnerabilities')\n",
    "\n",
    "    plt.setp([a.get_xticklabels() for a in f.axes[:-1]], visible=False)\n",
    "\n",
    "    plt.savefig('msr2019/img/vulnXYear'+suffix+'.pdf', dpi=200, bbox_inches='tight')\n",
    "    \n",
    "printVulnXYear(df_pos)\n",
    "printVulnXYear(df_pos_dedup,'DEDUP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populateInNvd(df):\n",
    "    count=0\n",
    "    for i, row in df.iterrows():\n",
    "        #print('https://nvd.nist.gov/vuln/detail/'+x)\n",
    "        count += 1\n",
    "        print('.', end = '')\n",
    "        if not count % 50:\n",
    "            print(' ' + str(count) + '\\n')\n",
    "        check_outcome = checkNVD(df.at[i,'vuln_id'])\n",
    "        df.at[i,'in_nvd'] = check_outcome\n",
    "    print('\\n')\n",
    "        \n",
    "populateInNvd(df_pos)\n",
    "populateInNvd(df_pos_dedup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_pos_dedup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countNVDStats(df,suffix=''):\n",
    "    vulns_all_count = df.vuln_id.nunique()\n",
    "    le.save('ALLVULNSCOUNT' +suffix,\n",
    "            vulns_all_count,\n",
    "            \"Number of distinct vulnerabilities in the dataset \" + suffix)\n",
    "\n",
    "    vulns_nvd_count = df[df['in_nvd'] == 1.0 ].vuln_id.nunique()\n",
    "    le.save('ALLVULNSINNVD'+suffix,\n",
    "        vulns_nvd_count,\n",
    "        'Number of vulnerabilities found in the NVD '  + suffix)\n",
    "\n",
    "    cves_no_nvd_count = vulns_all_count - vulns_nvd_count\n",
    "    le.save('CVENONVD'+suffix,\n",
    "        cves_no_nvd_count,\n",
    "        'Number of vulnerabilities that have a CVE but are not found in the NVD '  + suffix)\n",
    "\n",
    "    cves_count = len(df[df['vuln_id'].str.contains('CVE-')]['vuln_id'].unique())\n",
    "    le.save('VULNSNOCVE'+suffix,\n",
    "        vulns_all_count - cves_count,\n",
    "        'Number of vulnerabilities that do not have a CVE name '  + suffix)\n",
    "    \n",
    "countNVDStats(df_pos)\n",
    "countNVDStats(df_pos_dedup,'DEDUP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export stats to LaTeX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.to_file('msr2019/tex/data_from_notebook.tex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
